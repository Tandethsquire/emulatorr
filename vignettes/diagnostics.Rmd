---
title: "Emulator Specifications and Diagnostics"
output: rmarkdown::html_vignette
vignette : >
 %\VignetteIndexEntry{Emulator Specifications and Diagnostics}
 %\VignetteEngine{knitr::rmarkdown}
 \usepackage[utf8]{inputenc}
---

```{r echo = FALSE, message = FALSE}
library(emulatorr)
library(purrr)
```

## Further Specifications for Emulators

In the [introductory vignette](introduction.html) the steps for creating basic Bayes Linear emulators were discussed. We ignored a number of optional parameters that can be passed to the emulator construction, and indeed most of the work was done by the `emulator_from_data` helper function. Here we discuss why it may be useful to tweak the settings of the emulators by hand.

### Diagnostics
There are a number of different diagnostics that we can apply to emulators. First, we'll create and train the emulators as in the introductory vignette. We'll also bring in the validation data, `GillespieValidation`:

```{r}
input_data <- GillespieSIR
in_names <- c('aSI','aIR','aSR')
out_names <- c('nS','nI','nR')
ranges <- list(c(0.1,0.8), c(0,0.5), c(0,0.05))
base_ems <- emulator_from_data(input_data, in_names, out_names, ranges = ranges)
trained_ems_1 <- purrr::map(seq_along(base_ems), ~base_ems[[.x]]$bayes_adjust(GillespieSIR[,in_names], GillespieSIR[,out_names[[.x]]]))
validate <- GillespieValidation
```

The first diagnostic that we can consider is the comparison of the emulator results to the simulator results. For each point in te validation data set, we can find the emulator expectation and variance: we plot these against the simulator outputs for each of the inputs, with the error bars corresponding to 3 standard deviations.

```{r, fig.width = 7, fig.height = 3}
op <- par(mfrow = c(1,3))
for (i in 1:3) comparison_diagnostics(trained_ems_1[[i]], validate[, in_names], validate[, out_names[[i]]], list(c(0,1000), c(0,1000), output_name = out_names[[i]]))
par(op)
```

For a good set of emulators, we would expect that all emulator results from points in the validation set would have error bars containing the line f(x)=E[f(x)]. Points which do not satisfy this are highlighted in red: these can give an indication of problematic regions of the input space, or poor specifications in the emulators themselves. Running `comparison_diagnostics` returns any points that do not satisfy the requirement:

```{r}
comparison_diagnostics(trained_ems_1[[2]], validate[, in_names], validate[, 'nI'], plt=F)
comparison_diagnostics(trained_ems_1[[3]], validate[, in_names], validate[, 'nR'], plt=F)
```
The same point is proving hard to emulate for both the nI and nS emulators. The point is relatively close to the edges of the input space in all three directions, so may not warrant any concern: however, it could be worth running the simulator in a neighbourhood of this point to determine any strange behaviours.

The next diagnostic test is the standard errors of the emulators. For each point in the validation set, we can calculate (f(x)-E[f(x)])/sqrt(Var[f(x)]) as the standard error of the emulator result. We would expect that such errors fall, in general, between -3 and 3, and are approximately normal: conversely, we do not want the emulator standard errors to all be close to 0, as this would suggest over-trained emulators.

```{r, fig.width = 7, fig.height = 3}
op <- par(mfrow = c(1,3))
for (i in 1:3) standard_errors(trained_ems_1[[i]], validate[, in_names], validate[, out_names[[i]]], output_name = out_names[[i]])
par(op)
```
The standard errors for these emulators are not too bad. The nR emulator has a couple of points for which the errors exceed 3: again, we can examine this in more detail.

```{r}
standard_errors(trained_ems_1[[3]], validate[, in_names], validate[, 'nR'], plt=F)
```

Again, this is the same point that caused the problems in the previous diagnostics.

Finally, we can consider the emulator implausibility against the simulator implausibility. For a set of desired outputs, we can calculate the implausibility suggested by the emulator: we can also consider the implausibility suggested by the simulator in the same fashion. Points which are classified as non-implausible/implausible by *both* emulator and simulator are fine; points which are classed as implausible by the simulator but not the emulator are also fine (since successive waves of emulators can still rule these points out, if they are truly implausible); however, any point which is classified as implausible by the emulator but not the simulator should be treated with caution. Such points are highlighted in red in the plots.

```{r, fig.width = 7, fig.height = 3}
z <- list(list(val = 464, sigma = 14.86), list(val = 81, sigma = 5.48), list(val = 455, sigma = 14.13))
op <- par(mfrow = c(1,3))
for (i in 1:3) classification_error(trained_ems_1[[i]], validate[, in_names], validate[, out_names[[i]]], z[[i]], output_name = out_names[[i]])
par(op)
```
Here we see that the nS emulator is not as accurate as the previous diagnostics suggested. The other two have misclassifed a single point (in this case, different from the previous problem point), but there are quite a few misclassified points in the nS output. Looking at the two sets of implausibilities for these points:

```{r}
misclass <- classification_error(trained_ems_1[[1]], validate[, in_names], validate[, out_names[[1]]], z[[1]], plt = F)
c(apply(misclass, 1, trained_ems_1[[1]]$implausibility, z[[1]]), use.names = F)
map_dbl(validate[row.names(misclass),'nS'], ~sqrt((.x-z[[1]]$val)^2)/z[[1]]$sigma)
```
This could be down to a number of factors. It may be that we have mis-specified the observational error, and we actually have less uncertainty in this input; however, we would expect to have comparatively few implausible points from the simulator were that the case. It may instead be that the specifications for the emulator are incorrect - we could try to increase the variance of the emulator by tweaking sigma, or the correlation length. We could also introduce a *nugget* term: rather than the current correlation structure, which treats sigma as an overall multiplicative factor `sigma^2*c(x,x')`, we instead write the correlation structure u(x) as `u(x) = sigma^2*(1-delta)*c(x,x') + sigma^2*delta*I(x,x')`, where delta is some numeric factor and I(x,x') is an indicator function. This deflates the covariance between different points, while preserving the variance at a point.

```{r, fig.width=7, fig.height=7}
new_base_ems <- emulator_from_data(input_data, in_names, out_names, ranges = ranges, c_lengths = c(2/3, 1/3, 1/3), deltas = c(0.08522, 0, 0))
new_nS_em <- new_base_ems[[1]]$bayes_adjust(input_data[,in_names], input_data[,'nS'])
classification_error(new_nS_em, validate[,in_names], validate[,'nS'],z[[1]])
```
This has improved the classification issue, and checking the other plots shows that the remaining diagnostics are in line with expectations. We can go further by modifying the emulator specifications by hand to inflate the uncertainty; we will create a Correlator object with the desired structure (including the modifed correlation length and nugget term from above), but with a larger sigma, and use this in the base specifications for the emulator:

```{r, fig.width = 7, fig.height = 7}
new_covariance <- function(x,xp) 20000*(1-0.08522)*exp_sq(x,xp,2/3)+ifelse(abs(x-xp)<0.0001, 0.08522*20000, 0)
new_base_ems[[1]]$u <- Correlator$new(new_covariance, function(x) 0)
new_nS_em <- new_base_ems[[1]]$bayes_adjust(input_data[,in_names], input_data[,'nS'])
classification_error(new_nS_em, validate[,in_names], validate[,'nS'], z[[1]])
```

A final 'diagnostic' check is less on the structure of the emulators, but on the structure of the data itself. We may plot the output values against the inputs for each input/output pair to detect any underlying trends. If any can be found, we would expect these to be respected by any valid set of emulators. We will demonstrate this with the new-and-improved emulator for the nS output, and modify the colour based on the aIR or aSR value at the point.

```{r, fig.width = 7, fig.height = 7}
grd <- expand.grid(seq(0.1,0.8,length.out=30), seq(0,0.5, length.out=30))
emulator_out <- apply(grd, 1, function(x) new_nS_em$get_exp(c(x,0.025)))
colours <- c('#000000', '#222222', '#444444', '#666666', '#888888', '#aaaaaa')
plot(grd[,1], emulator_out, pch = 16, col = colours[floor(10*grd[,2])+1], xlab = "aSI", ylab = "nS")
grd2 <- expand.grid(seq(0.1,0.8, length.out=30), seq(0, 0.05, length.out=30))
emulator_out_2 <- apply(grd, 1, function(x) new_nS_em$get_exp(c(x[[1]], 0.25, x[[2]])))
plot(grd[,1], emulator_out, pch = 16, col = colours[floor(100*grd[,2])+1], xlab = "aSI", ylab = "nS")
```

There is a clear trend here: increasing infection rate reduces the number of susceptible people in the population after some time has passed. This should be expected, but the difference in variation for the varying aIR and aSR parameters is perhaps more surprising. In more complicated models, plots like this can give a useful insight into the behaviour of the data, and indeed how well-behaved the output of the emulator is.
